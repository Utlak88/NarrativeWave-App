{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NarrativeWave App.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMYfwQ86WzhUzDZP5QMY1st",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utlak88/NarrativeWave-App/blob/main/NarrativeWave_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5OoaCGcVIZ3"
      },
      "source": [
        "# **Narrative Wave App**\n",
        "\n",
        "This Notebook documents the steps to load and process csv data in prepartion for plotting in a Django application. The data will ultimately be stored in parquet format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwRIxThFQuCl"
      },
      "source": [
        "PySpark and pandas will be needed to analyze and manipulate the provided dataset.\n",
        "\n",
        "PySpark requires a series of steps to install. First install Java Virtual Machine and Apache Spark with Hadoop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woo22S-1Qr0t"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SZi6UlLX90l"
      },
      "source": [
        "Next install findspark to locate Spark within the system and import as a library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v71wX5YkX96Q"
      },
      "source": [
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azOgW4OiXSAI"
      },
      "source": [
        "Set environment paths to allow PySpark to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOqwruABXSQr"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIrPIDHMXZIj"
      },
      "source": [
        "Locate Spark within the system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt2h5oGZXYwF"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV1x5xT30u1T"
      },
      "source": [
        "Next import PySpark and pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6eEGkt20uFZ"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.types import *\n",
        "import pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIGcZ76A1Azm"
      },
      "source": [
        "Initiating a Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNuHOxMd1JsN"
      },
      "source": [
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ACQqDru1K7z"
      },
      "source": [
        "Data is initially provided in csv format. These files can be read into a variable with the following line, which requires that all csv files are saved in the same directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSSR3o7fH2qu"
      },
      "source": [
        "csv_data = spark.read.option(\"header\", True).csv('/content/csv_files/*.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TER8i4axUfJD"
      },
      "source": [
        "The dataset can be visualized by converting to a pandas dataframe, which indicates that the dataset is structured in long format.\n",
        "\n",
        "The initial data structure consists of three columns labeled 'timestamp', 'tag', and 'value'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OL_ShkQUiRi"
      },
      "source": [
        "csv_df = csv_data.toPandas()\n",
        "csv_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uze8ygAFKa7T"
      },
      "source": [
        "tag is provided in the format '/​narrativewave​/​WTG13​/Amb_WindSpeed_Avg' where 'WTG13' is labeled as 'asset' and 'Amb_WindSpeed_Avg' is labeled as 'column'. These elements need to be extracted from the tag, which is accomplished by the two functions below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJeFRehvLA_p"
      },
      "source": [
        "def asset_extract(tag):\n",
        "    return tag.split('/')[2]\n",
        "\n",
        "def column_extract(tag):\n",
        "    return tag.split('/')[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IazcnvdLDsc"
      },
      "source": [
        "Spark requires that the two above functions be expressed as user defined functions (UDF)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEU6R3V7LYAw"
      },
      "source": [
        "data_asset_extract_udf = f.udf(asset_extract, StringType())\n",
        "data_column_extract_udf = f.udf(column_extract, StringType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG8Kqt1QLh_G"
      },
      "source": [
        "Now assets and columns can be extracted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TssdKK1sLxsP"
      },
      "source": [
        "data_asset_extracted = csv_data.withColumn('asset', data_asset_extract_udf('tag'))\n",
        "data_asset_column_extracted = data_asset_extracted.withColumn('column', data_column_extract_udf('tag'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_eUI2h1LvFV"
      },
      "source": [
        "With assets and columns extracted, the tag column is no longer needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avSKCpV2MGcC"
      },
      "source": [
        "data_only_extracted = data_asset_column_extracted.drop('tag')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Raj--AtIcLWw"
      },
      "source": [
        "Visualize the processed dataset as a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkE89H1hcLhD"
      },
      "source": [
        "data_only_extracted_df = data_only_extracted.toPandas()\n",
        "data_only_extracted_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2zpFYvtMbaQ"
      },
      "source": [
        "Entries in the 'value' column associated with 'TimeStamp' in the column labeled as 'column' are non-numeric. As the goal is to plot data values, rows containing 'TimeStamp' need to be removed.\n",
        "\n",
        "This does not impact the dataset as timestamps are also included in the 'timestamp' column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh6cWkU2QGEV"
      },
      "source": [
        "data_no_time_stamp_column = data_only_extracted.filter(f.col('column') != 'TimeStamp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akA_lBO4fhNW"
      },
      "source": [
        "By default, all dataset entries are strings, which need to be converted to intended types.\n",
        "\n",
        "Entries in the 'value' column, for instance, need to be converted to a numeric type. It will be helpful to first view a section from 'value'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYxkKl9AfhV7"
      },
      "source": [
        "data_no_time_stamp_column_df = data_no_time_stamp_column.toPandas()\n",
        "data_no_time_stamp_column_df['value'][:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dWVdc_5QGwf"
      },
      "source": [
        "This sampling indicates that both integer and floats exist as values.\n",
        "\n",
        "An additional validation can be performed by using a regex search to confirm that no characters exist in the 'value' column. Note, this search excludes 'E' to bypass numeric values reported in scientific notation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKLW-yT8jOIf"
      },
      "source": [
        "data_no_time_stamp_column_df['value'].str.contains(r'[a-zA-DF-Z]').unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHBCMAV7mmWT"
      },
      "source": [
        "The regex search indicates that all values are numeric. As such, these values can now be converted to float type, which will properly account for all number formats in the 'value' column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBYIs5JqQXUm"
      },
      "source": [
        "data_values_float_type = data_no_time_stamp_column.withColumn('value', data_no_time_stamp_column.value.cast(FloatType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g4rO8ymQcTD"
      },
      "source": [
        "Similarly, timestamp strings need to be converted to timestamp format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhPkbogGQlGH"
      },
      "source": [
        "data_timestamp_type = data_values_float_type.withColumn('timestamp', f.to_timestamp('timestamp'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbajZO9fQmLj"
      },
      "source": [
        "Month and year can now be extracted from the timestamps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ5sT3I7Us39"
      },
      "source": [
        "data_month_year_columns = data_timestamp_type.withColumn('month', f.month('timestamp')).withColumn('Year', f.year('timestamp'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4yClR3Tn31i"
      },
      "source": [
        "It is useful at this point to visualize the updated dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppG1oCm0n4CQ"
      },
      "source": [
        "data_month_year_columns_df = data_month_year_columns.toPandas()\n",
        "data_month_year_columns_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At4GWeRzoZPd"
      },
      "source": [
        "Converted data types can also be confirmed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iioVXpadoZVf"
      },
      "source": [
        "data_month_year_columns_df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcObLyCPUynu"
      },
      "source": [
        "Data types have been defined as intended.\n",
        "\n",
        "The dataset now needs to be converted to wide format in preparation for storing in parquet format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijNFUTjvVL5n"
      },
      "source": [
        "data_values_wide_format = data_month_year_columns.groupby('asset', 'timestamp', 'month', 'year').pivot('column').agg(f.sum('value'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME20E7mTV03d"
      },
      "source": [
        "Visualizing wide-structured dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd75VD1hV1J5"
      },
      "source": [
        "data_values_wide_format_df = data_values_wide_format.toPandas()\n",
        "data_values_wide_format_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxDrnWq8V5Ig"
      },
      "source": [
        "There is the potential that a percentage of column values may be missing. Let's verify if that is the case, and if so, where these missing values are located."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PneISFAxV5XQ"
      },
      "source": [
        "pandas.set_option('display.max_row', None)\n",
        "data_values_wide_format_df.isnull().sum().sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6rG7li5tEjL"
      },
      "source": [
        "The above table indicates that missing values exist in 14 columns, all of which except one are prediction categories. These could be addressed by removing from the dataset or employing a form of interpolation if bracketed by values at close timestamps.\n",
        "\n",
        "The current effort will not alter the missing values as the end goal is to demonstrate that a REST API service can be consumed and does not request that the data be plotted. The missing data will not be impactful if the dataset is not graphically visualized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIxIIG7hVSV6"
      },
      "source": [
        "The dataset can now be stored in a directory in parquet format partitioned by asset, year, and month."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABvNHPbEVVIG"
      },
      "source": [
        "data_values_wide_format.write.partitionBy('asset', 'year', 'month').parquet('/content/parquet_output/')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}